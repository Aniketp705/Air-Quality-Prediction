{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from scipy.stats import randint\n",
        "import joblib\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"/content/city_day.csv\")\n",
        "\n",
        "# Drop 'City' if present\n",
        "if 'City' in data.columns:\n",
        "    data = data.drop('City', axis=1)\n",
        "\n",
        "# Impute missing numerical values with median\n",
        "numerical_cols = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
        "                  'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene']\n",
        "for col in numerical_cols:\n",
        "    if col in data.columns:\n",
        "        data[col] = data[col].fillna(data[col].median())\n",
        "\n",
        "# Impute missing categorical values\n",
        "if 'AQI_Bucket' in data.columns:\n",
        "    data['AQI_Bucket'] = data['AQI_Bucket'].fillna(data['AQI_Bucket'].mode()[0])\n",
        "\n",
        "# Drop AQI column as it's the target we want to predict indirectly\n",
        "if 'AQI' in data.columns:\n",
        "    data = data.drop('AQI', axis=1)\n",
        "\n",
        "# Extract date features if 'Date' exists\n",
        "if 'Date' in data.columns:\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data['Year'] = data['Date'].dt.year\n",
        "    data['Month'] = data['Date'].dt.month\n",
        "    data['Day'] = data['Date'].dt.day\n",
        "    data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
        "    data = data.drop('Date', axis=1)\n",
        "\n",
        "# Drop duplicates\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "target_variable = 'AQI_Bucket'\n",
        "\n",
        "if target_variable in data.columns:\n",
        "    X = data.drop(target_variable, axis=1)\n",
        "    y = data[target_variable]\n",
        "\n",
        "    # Split dataset with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Scale numerical features only\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled_numeric = scaler.fit_transform(X_train[numerical_cols])\n",
        "    X_test_scaled_numeric = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "    # Combine scaled numeric features with date features\n",
        "    date_cols = ['Year', 'Month', 'Day', 'DayOfWeek']\n",
        "    X_train_final = np.concatenate([X_train_scaled_numeric, X_train[date_cols].values], axis=1)\n",
        "    X_test_final = np.concatenate([X_test_scaled_numeric, X_test[date_cols].values], axis=1)\n",
        "\n",
        "    # Define hyperparameter distribution for RandomizedSearchCV\n",
        "    param_dist = {\n",
        "        'n_estimators': randint(50, 300),\n",
        "        'max_depth': randint(5, 30),\n",
        "        'min_samples_split': randint(2, 10),\n",
        "        'min_samples_leaf': randint(1, 5),\n",
        "        'class_weight': ['balanced']\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=30,              # Number of parameter settings sampled\n",
        "        cv=3,                  # 3-fold cross-validation\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,             # Use all CPU cores\n",
        "        verbose=2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Run hyperparameter tuning\n",
        "    random_search.fit(X_train_final, y_train)\n",
        "\n",
        "    print(\"Best hyperparameters:\", random_search.best_params_)\n",
        "    print(\"Best cross-validation accuracy:\", random_search.best_score_)\n",
        "\n",
        "    best_rf = random_search.best_estimator_\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred = best_rf.predict(X_test_final)\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Test set accuracy: {test_acc:.4f}\")\n",
        "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # Save model and scaler\n",
        "    joblib.dump(best_rf, \"aqi_random_forest_best_model.pkl\")\n",
        "    joblib.dump(scaler, \"aqi_scaler.pkl\")\n",
        "\n",
        "else:\n",
        "    print(f\"Target variable '{target_variable}' not found in the DataFrame.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZrdRFiCaiS_",
        "outputId": "3f23f869-59e4-4c09-e06c-5530ebad7207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "Best hyperparameters: {'class_weight': 'balanced', 'max_depth': 26, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 138}\n",
            "Best cross-validation accuracy: 0.8165856207895273\n",
            "Test set accuracy: 0.8207\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        Good       0.78      0.68      0.73       268\n",
            "    Moderate       0.87      0.87      0.87      2606\n",
            "        Poor       0.69      0.65      0.67       556\n",
            "Satisfactory       0.81      0.85      0.83      1645\n",
            "      Severe       0.82      0.78      0.80       268\n",
            "   Very Poor       0.73      0.78      0.75       467\n",
            "\n",
            "    accuracy                           0.82      5810\n",
            "   macro avg       0.78      0.77      0.77      5810\n",
            "weighted avg       0.82      0.82      0.82      5810\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_rf, \"aqi_random_forest_best_model.pkl\")\n",
        "joblib.dump(scaler, \"aqi_scaler.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUf9425Lf2sC",
        "outputId": "70b2cec3-27d5-4791-cd60-4739193ed0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aqi_scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from scipy.stats import randint\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"/content/city_day.csv\")\n",
        "\n",
        "# Drop 'City' if present\n",
        "if 'City' in data.columns:\n",
        "    data = data.drop('City', axis=1)\n",
        "\n",
        "# Impute missing numerical values with median\n",
        "numerical_cols = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
        "                  'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene']\n",
        "for col in numerical_cols:\n",
        "    if col in data.columns:\n",
        "        data[col] = data[col].fillna(data[col].median())\n",
        "\n",
        "# Impute missing categorical values\n",
        "if 'AQI_Bucket' in data.columns:\n",
        "    data['AQI_Bucket'] = data['AQI_Bucket'].fillna(data['AQI_Bucket'].mode()[0])\n",
        "\n",
        "# Drop AQI column as it's the target we want to predict indirectly\n",
        "if 'AQI' in data.columns:\n",
        "    data = data.drop('AQI', axis=1)\n",
        "\n",
        "# Extract date features if 'Date' exists\n",
        "if 'Date' in data.columns:\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data['Year'] = data['Date'].dt.year\n",
        "    data['Month'] = data['Date'].dt.month\n",
        "    data['Day'] = data['Date'].dt.day\n",
        "    data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
        "    data = data.drop('Date', axis=1)\n",
        "\n",
        "# Drop duplicates\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "target_variable = 'AQI_Bucket'\n",
        "if target_variable in data.columns:\n",
        "    X = data.drop(target_variable, axis=1)\n",
        "    y = data[target_variable]\n",
        "\n",
        "    # Split dataset with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Identify numerical and date columns for separate handling\n",
        "    numerical_cols_present = [col for col in numerical_cols if col in X_train.columns]\n",
        "    date_cols_present = ['Year', 'Month', 'Day', 'DayOfWeek']\n",
        "    date_cols_present = [col for col in date_cols_present if col in X_train.columns]\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "C7-q1YNL73d2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numerical features only\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_numeric = scaler.fit_transform(X_train[numerical_cols_present])\n",
        "X_test_scaled_numeric = scaler.transform(X_test[numerical_cols_present])\n",
        "\n",
        "# Combine scaled numeric features with date features\n",
        "X_train_final = np.concatenate([X_train_scaled_numeric, X_train[date_cols_present].values], axis=1)\n",
        "X_test_final = np.concatenate([X_test_scaled_numeric, X_test[date_cols_present].values], axis=1)\n",
        "\n",
        "# Define hyperparameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "  'n_estimators': randint(50, 300),\n",
        "  'max_depth': randint(5, 30),\n",
        "  'min_samples_split': randint(2, 10),\n",
        "  'min_samples_leaf': randint(1, 5),\n",
        "  'class_weight': ['balanced']\n",
        "}\n"
      ],
      "metadata": {
        "id": "x1-FcEbg0oUA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "  estimator=rf,\n",
        "  param_distributions=param_dist,\n",
        "  n_iter=30,              # Number of parameter settings sampled\n",
        "  cv=3,                  # 3-fold cross-validation\n",
        "  scoring='accuracy',\n",
        "  n_jobs=-1,             # Use all CPU cores\n",
        "  verbose=2,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "# Run hyperparameter tuning\n",
        "random_search.fit(X_train_final, y_train)\n",
        "\n",
        "print(\"Best hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", random_search.best_score_)\n",
        "\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_rf.predict(X_test_final)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Save model and scaler\n",
        "joblib.dump(best_rf, \"aqi_random_forest_best_model.pkl\")\n",
        "joblib.dump(scaler, \"aqi_scaler.pkl\")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the total training time\n",
        "training_time = end_time - start_time\n",
        "print(f\"\\nTotal training time: {training_time:.2f} seconds\")\n",
        "minutes = int(training_time // 60)\n",
        "seconds = int(training_time % 60)\n",
        "print(f\"Total training time: {minutes} minutes and {seconds} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHKJMfv00xam",
        "outputId": "2aa1527b-8501-4c10-f259-a729c490fce0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "Best hyperparameters: {'class_weight': 'balanced', 'max_depth': 26, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 138}\n",
            "Best cross-validation accuracy: 0.8165856207895273\n",
            "Test set accuracy: 0.8207\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        Good       0.78      0.68      0.73       268\n",
            "    Moderate       0.87      0.87      0.87      2606\n",
            "        Poor       0.69      0.65      0.67       556\n",
            "Satisfactory       0.81      0.85      0.83      1645\n",
            "      Severe       0.82      0.78      0.80       268\n",
            "   Very Poor       0.73      0.78      0.75       467\n",
            "\n",
            "    accuracy                           0.82      5810\n",
            "   macro avg       0.78      0.77      0.77      5810\n",
            "weighted avg       0.82      0.82      0.82      5810\n",
            "\n",
            "\n",
            "Total training time: 553.02 seconds\n",
            "Total training time: 9 minutes and 13 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oVOwZFH081q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}